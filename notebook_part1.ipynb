{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXp4wevFldxS"
      },
      "source": [
        "# TODO: clean up imports\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "from torch.optim.lr_scheduler import MultiStepLR"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKiuqFffl0KB"
      },
      "source": [
        "# TODO: check parameters, code >>> paper \n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 128 # TODO: 95% right\n",
        "test_batch_size = 10 # TODO: probably bigger, doesn't really impact performance\n",
        "classes_group = 10 # TODO: check with 2, 5 and 50 as well\n",
        "epochs = 70 # TODO: should be 60, 200 doesn't change performance meaningfully\n",
        "_lr = 2 #0.1 # TODO: 2 is way too high but iCarl uses it, what's in the code?\n",
        "wd = 1e-4 # TODO: 99% right #1\n",
        "mom = 0.9 # TODO: 99.99% right\n",
        "pic_height = 32 # TODO: probably not needed"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90UQiONWli9r",
        "outputId": "04a87963-7e34-4730-91bd-56db1164d381"
      },
      "source": [
        "# Once we have finished repository\n",
        "'''\n",
        "if not os.path.isdir('./...'):\n",
        "  !git clone https://github.com/...\n",
        "\n",
        "DATA_DIR = './...'\n",
        "from ... .data import Cifar100\n",
        "'''\n",
        "\n",
        "# Load the dataset with custom class\n",
        "from data import Cifar100\n",
        "DATA_DIR = '.'\n",
        "training_data = Cifar100(DATA_DIR, True, transform=transforms.ToTensor())\n",
        "test_data = Cifar100(DATA_DIR, False, transform=transforms.ToTensor())\n",
        "\n",
        "print(f'Train Dataset: {len(training_data)}')\n",
        "print(f'Test Dataset: {len(test_data)}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Dataset: 50000\n",
            "Test Dataset: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aldv70nxGu66"
      },
      "source": [
        "# TODO: Test vs resnet32 (should be comparable: source iCarl, pag. 6) (use _resnet() (careful there's an underscore!) from pytorch, just need  to find the right list for \"layers\" argument)\n",
        "\n",
        "from resnet_cifar import resnet32\n",
        "model = resnet32(num_classes=classes_group)\n",
        "model = model.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss() #BCE + sogmoid all in one\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=_lr, weight_decay=wd, momentum = mom)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWdrlDtu4iHs"
      },
      "source": [
        "train_batches, test_batches = [], []\n",
        "for i in range(10):\n",
        "  train_batch_indexes = training_data.__BatchIndexes__(i)\n",
        "  \n",
        "  test_batch_indexes = []\n",
        "  for j in range(i+1):\n",
        "    test_indexes = test_data.__BatchIndexes__(j)\n",
        "    test_batch_indexes += test_indexes\n",
        "\n",
        "  train_batch = Subset(training_data,train_batch_indexes)\n",
        "  test_batch = Subset(test_data,test_batch_indexes)\n",
        "\n",
        "  train_batches.append(train_batch)\n",
        "  test_batches.append(test_batch)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WmIFJ5mnTTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9440eb5-bfcc-4ce5-bcc9-ccfdffcf3284"
      },
      "source": [
        "# TODO: clean up methods, should probably implement eval method in utils.py or similar, see below\n",
        "\n",
        "train_loader = DataLoader(train_batches[0], batch_size, True, drop_last=True)\n",
        "from data import augmentate \n",
        "model.train()\n",
        "torch.set_printoptions(edgeitems=5) # just for debug purpouses\n",
        "\n",
        "# TODO: check scheduler parameters, is it even needed?\n",
        "scheduler = MultiStepLR(optimizer, milestones=[48, 62], gamma=0.2)\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  print(f\"Epoch: {epoch+1:02} \", end=\"\")\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(train_loader, 0): # batch is just the index\n",
        "  #for X, y in train_loader\n",
        "    X, y = X.to(device), torch.from_numpy(np.array(y)).to(device)\n",
        "    # Data augmentation\n",
        "    X = torch.stack([augmentate(image) for image in X])\n",
        "    #X = torch.stack([image for image in X ])\n",
        "    optimizer.zero_grad() # needs to be before predictions\n",
        " \n",
        "    # Compute prediction loss\n",
        "    outputs = model(X) #pred.size([128, 10])\n",
        "    pred = torch.argmax(outputs.data, 1)\n",
        "    oh_y = torch.zeros(outputs.shape[0], outputs.shape[1])\n",
        "    oh_y[range(oh_y.shape[0]), y.long()]=1\n",
        "    loss = loss_fn(outputs, oh_y.to(device)) \n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update loss\n",
        "    train_loss += loss\n",
        "\n",
        "  print(f\"loss: {train_loss/batch:.4f}, \", end = \"\")\n",
        "  scheduler.step()\n",
        "\n",
        "  # eval per epoch\n",
        "  # TODO: probably needs to move to a utils.py \n",
        "  with torch.no_grad():\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    model.eval()\n",
        "    # TODO: be consistant with the way of handling dataloaders (unpack inside of for cycle or during for inizialization)\n",
        "    for data in train_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        wrn_outputs = model(images)\n",
        "        outputs = wrn_outputs\n",
        "        predicted = torch.argmax(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_accuracy = correct / total\n",
        "    epoch_accuracy = round(100 * epoch_accuracy, 4)\n",
        "\n",
        "  print(f\"acc: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 loss: 0.4517, acc: 10.96%\n",
            "Epoch: 02 loss: 0.3321, acc: 21.77%\n",
            "Epoch: 03 loss: 0.3216, acc: 21.89%\n",
            "Epoch: 04 loss: 0.3105, acc: 11.52%\n",
            "Epoch: 05 loss: 0.3011, acc: 26.18%\n",
            "Epoch: 06 loss: 0.2882, acc: 29.59%\n",
            "Epoch: 07 loss: 0.2910, acc: 32.43%\n",
            "Epoch: 08 loss: 0.2857, acc: 18.65%\n",
            "Epoch: 09 loss: 0.2830, acc: 38.98%\n",
            "Epoch: 10 loss: 0.2789, acc: 15.99%\n",
            "Epoch: 11 loss: 0.2797, acc: 27.74%\n",
            "Epoch: 12 loss: 0.2740, acc: 43.43%\n",
            "Epoch: 13 loss: 0.2766, acc: 26.28%\n",
            "Epoch: 14 loss: 0.2694, acc: 28.61%\n",
            "Epoch: 15 loss: 0.2677, acc: 12.84%\n",
            "Epoch: 16 loss: 0.2698, acc: 20.21%\n",
            "Epoch: 17 loss: 0.2642, acc: 27.88%\n",
            "Epoch: 18 loss: 0.2585, acc: 29.99%\n",
            "Epoch: 19 loss: 0.2538, acc: 11.26%\n",
            "Epoch: 20 loss: 0.2530, acc: 26.92%\n",
            "Epoch: 21 loss: 0.2459, acc: 12.78%\n",
            "Epoch: 22 loss: 0.2501, acc: 38.72%\n",
            "Epoch: 23 loss: 0.2474, acc: 12.10%\n",
            "Epoch: 24 loss: 0.2472, acc: 12.06%\n",
            "Epoch: 25 loss: 0.2501, acc: 40.28%\n",
            "Epoch: 26 loss: 0.2474, acc: 25.96%\n",
            "Epoch: 27 loss: 0.2455, acc: 40.34%\n",
            "Epoch: 28 loss: 0.2370, acc: 41.29%\n",
            "Epoch: 29 loss: 0.2351, acc: 26.64%\n",
            "Epoch: 30 loss: 0.2400, acc: 44.33%\n",
            "Epoch: 31 loss: 0.2332, acc: 39.88%\n",
            "Epoch: 32 loss: 0.2342, acc: 31.63%\n",
            "Epoch: 33 loss: 0.2313, acc: 38.80%\n",
            "Epoch: 34 loss: 0.2349, acc: 53.06%\n",
            "Epoch: 35 loss: 0.2347, acc: 38.56%\n",
            "Epoch: 36 loss: 0.2300, acc: 55.67%\n",
            "Epoch: 37 loss: 0.2346, acc: 15.62%\n",
            "Epoch: 38 loss: 0.2257, acc: 42.83%\n",
            "Epoch: 39 loss: 0.2259, acc: 51.60%\n",
            "Epoch: 40 loss: 0.2286, acc: 40.79%\n",
            "Epoch: 41 loss: 0.2281, acc: 61.84%\n",
            "Epoch: 42 loss: 0.2193, acc: 43.37%\n",
            "Epoch: 43 loss: 0.2201, acc: 18.63%\n",
            "Epoch: 44 loss: 0.2247, acc: 56.85%\n",
            "Epoch: 45 loss: 0.2173, acc: 10.42%\n",
            "Epoch: 46 loss: 0.2161, acc: 16.65%\n",
            "Epoch: 47 loss: 0.2202, acc: 53.69%\n",
            "Epoch: 48 loss: 0.2171, acc: 39.82%\n",
            "Epoch: 49 loss: 0.2032, acc: 74.60%\n",
            "Epoch: 50 loss: 0.1918, acc: 71.05%\n",
            "Epoch: 51 loss: 0.1937, acc: 76.20%\n",
            "Epoch: 52 loss: 0.1903, acc: 75.12%\n",
            "Epoch: 53 loss: 0.1873, acc: 73.86%\n",
            "Epoch: 54 loss: 0.1879, acc: 81.77%\n",
            "Epoch: 55 loss: 0.1890, acc: 76.52%\n",
            "Epoch: 56 loss: 0.1870, acc: 79.13%\n",
            "Epoch: 57 loss: 0.1878, acc: 75.52%\n",
            "Epoch: 58 loss: 0.1827, acc: 63.04%\n",
            "Epoch: 59 loss: 0.1885, acc: 79.57%\n",
            "Epoch: 60 loss: 0.1813, acc: 82.09%\n",
            "Epoch: 61 loss: 0.1795, acc: 72.86%\n",
            "Epoch: 62 loss: 0.1845, acc: 76.78%\n",
            "Epoch: 63 loss: 0.1753, acc: 86.40%\n",
            "Epoch: 64 loss: 0.1748, acc: 87.94%\n",
            "Epoch: 65 loss: 0.1725, acc: 87.18%\n",
            "Epoch: 66 loss: 0.1699, acc: 88.44%\n",
            "Epoch: 67 loss: 0.1679, acc: 85.66%\n",
            "Epoch: 68 loss: 0.1687, acc: 87.62%\n",
            "Epoch: 69 loss: 0.1694, acc: 85.28%\n",
            "Epoch: 70 loss: 0.1705, acc: 85.90%\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IuDnwZmXDg5"
      },
      "source": [
        "# TODO: evaluate_model should be moved to a \"test_model\" and in utils.py, needs to be cleaned up. Use a single way of computing accuracy, needs to be consistent with eval_per_epoch in training\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "  model.eval()\n",
        "  correct, wrong = 0, 0\n",
        "  # TODO: check device, clean variables\n",
        "  tot_pred, ground_truth, fin_probs,tot_pred2, = list(), list(), list(), list()\n",
        "  for i, (inputs, targets) in enumerate(test_dl):\n",
        "    inputs, targets = inputs.to(device), targets.to('cpu')\n",
        "    ground_truth.extend(targets.numpy().tolist())\n",
        "    with torch.no_grad():\n",
        "      pred = model(inputs)\n",
        "      pred = pred.cpu()\n",
        "      pred = torch.argmax(pred.data, 1)\n",
        "      tot_pred.extend(*pred.reshape((1,pred.shape[0])).numpy().tolist())\n",
        "      for index, p in enumerate(pred):\n",
        "        if p == targets[index]:\n",
        "          correct += 1\n",
        "        else:\n",
        "          wrong += 1\n",
        "\n",
        "  # TODO: improve outputs\n",
        "  acc = correct/(correct+wrong)\n",
        "  print(f'sk2: {accuracy_score(ground_truth, tot_pred)}')\n",
        "  return acc\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELqwYfBrX4yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f76425-8047-4ef4-f887-f9b0bd87b1a5"
      },
      "source": [
        "test_loader = DataLoader(test_batches[0], batch_size, True)\n",
        "evaluate_model(test_loader, model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sk2: 0.806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}